{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset contains several telemarketing efforts carried out between May 2008 and November 2010, totaling 17 marketing initiatives, according to the Materials and Methods section of the UCI paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the provided path\n",
    "df = pd.read_csv('data/bank-additional/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows to confirm load success\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect info and datatypes\n",
    "df.info()\n",
    "\n",
    "# Check missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Objective\n",
    "\n",
    "#To predict whether a client will subscribe to a term deposit (target y) before making a marketing call, so the bank can:\n",
    "\n",
    "        #Reduce unnecessary calls\n",
    "\n",
    "        #Increase efficiency\n",
    "\n",
    "        #Improve customer targeting\n",
    "\n",
    "#This is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the df to avoid altering the original\n",
    "df_model = df.copy()\n",
    "\n",
    "# Remove 'duration' as it leaks outcome info\n",
    "df_model = df_model.drop(columns=['duration'])\n",
    "\n",
    "# Convert target 'y' to binary 1/0\n",
    "df_model['y'] = df_model['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Identify features\n",
    "X = df_model.drop(columns=['y'])\n",
    "y = df_model['y']\n",
    "\n",
    "# Identify column types\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_features, categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratify to preserve class ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dummy classifier predicting majority class\n",
    "baseline = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "y_pred_base = baseline.predict(X_test)\n",
    "\n",
    "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Baseline accuracy â‰ˆ 0.88, since most customers say \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Build pipeline\n",
    "log_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "log_reg_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = log_reg_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline for each model\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocess', preprocess),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Time the training\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    \n",
    "    train_time = round(end - start, 4)\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    \n",
    "    results.append([name, train_time, train_acc, test_acc])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Train Time\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search + Hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Decision Tree tuning\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 10, None],\n",
    "    'model__min_samples_split': [2, 10, 20]\n",
    "}\n",
    "\n",
    "tree_pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(tree_pipeline, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all four models and their hyperparameter grids\n",
    "# We are wraping each model in a Pipeline with the same preprocessing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models_param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "        ]),\n",
    "        # C controls regularization strength\n",
    "        \"param_grid\": {\n",
    "            \"model__C\": [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", KNeighborsClassifier())\n",
    "        ]),\n",
    "        # n_neighbors and weights are key KNN hyperparameters\n",
    "        \"param_grid\": {\n",
    "            \"model__n_neighbors\": [5, 15, 25],\n",
    "            \"model__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"))\n",
    "        ]),\n",
    "        # Depth and min_samples_leaf control complexity / overfitting\n",
    "        \"param_grid\": {\n",
    "            \"model__max_depth\": [3, 5, 10, None],\n",
    "            \"model__min_samples_leaf\": [1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"model\", SVC(kernel=\"rbf\", class_weight=\"balanced\"))\n",
    "        ]),\n",
    "        # C and gamma are core SVM hyperparameters\n",
    "        \"param_grid\": {\n",
    "            \"model__C\": [0.1, 1.0, 10.0],\n",
    "            \"model__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a tuned model\n",
    "def evaluate_tuned_model(name, grid_search, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Prints metrics for a tuned model and returns them in a dict.\n",
    "    \"\"\"\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    \n",
    "    # Use the best model to make predictions on the test set\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    # Try to compute ROC-AUC; depending on the model we use predict_proba or decision_function\n",
    "    if hasattr(best_estimator, \"predict_proba\"):\n",
    "        y_proba = best_estimator.predict_proba(X_test)[:, 1]\n",
    "        roc = roc_auc_score(y_test, y_proba)\n",
    "    elif hasattr(best_estimator, \"decision_function\"):\n",
    "        scores = best_estimator.decision_function(X_test)\n",
    "        roc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc = float(\"nan\")\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(\"Best Params:\", grid_search.best_params_)\n",
    "    print(f\"CV Best {grid_search.scoring}: {grid_search.best_score_:.3f}\")\n",
    "    print(\"--- Test Set Performance ---\")\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n",
    "    print(f\"ROC-AUC:   {roc:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Best Params\": grid_search.best_params_,\n",
    "        \"CV Best Score\": grid_search.best_score_,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Test Precision\": prec,\n",
    "        \"Test Recall\": rec,\n",
    "        \"Test F1\": f1,\n",
    "        \"Test ROC-AUC\": roc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV for all models and collect results\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring_metric = \"f1\"  # we optimize F1 because of class imbalance\n",
    "\n",
    "tuned_results = []\n",
    "\n",
    "for name, config in models_param_grids.items():\n",
    "    print(f\"\\n### Tuning {name} ###\")\n",
    "    \n",
    "    pipeline = config[\"pipeline\"]\n",
    "    param_grid = config[\"param_grid\"]\n",
    "    \n",
    "    # Wrap the pipeline in GridSearchCV\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring_metric,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Time the grid search to compare training time\n",
    "    start_time = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Training + grid search time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate tuned model on the test set\n",
    "    result = evaluate_tuned_model(name, grid, X_test, y_test)\n",
    "    result[\"Train+Tune Time (s)\"] = round(end_time - start_time, 2)\n",
    "    tuned_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize all tuned model results in a DataFrame\n",
    "\n",
    "tuned_results_df = pd.DataFrame(tuned_results)\n",
    "tuned_results_df\n",
    "\n",
    "# Sort by F1-score (descending)\n",
    "tuned_results_df.sort_values(by=\"Test F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Target variable distribution plot and save to PNG\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"y\", data=df)\n",
    "plt.title(\"Target Variable Distribution (y)\")\n",
    "plt.xlabel(\"Subscribed to term deposit?\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/target_distribution.png\", dpi=300)\n",
    "plt.close()  # Close figure so it doesn't show multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target distribution plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Target variable distribution plot and save to PNG\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"y\", data=df)\n",
    "plt.title(\"Target Variable Distribution (y)\")\n",
    "plt.xlabel(\"Subscribed to term deposit?\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/target_distribution.png\", dpi=300)\n",
    "plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df[\"age\"], bins=30, kde=True)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/age_distribution.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of \"yes\" by job\n",
    "job_y = pd.crosstab(df[\"job\"], df[\"y\"], normalize=\"index\")[\"yes\"].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "job_y.plot(kind=\"bar\")\n",
    "plt.title(\"Proportion of 'yes' by Job\")\n",
    "plt.ylabel(\"Proportion of yes\")\n",
    "plt.xlabel(\"Job\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/proportion_yes_by_job.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model comparison plot (F1-score)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.barplot(x=\"Model\", y=\"Test F1\", data=tuned_results_df)\n",
    "plt.title(\"Model Comparison by F1-score (Tuned Models)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Test F1-score\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/model_comparison_f1.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
